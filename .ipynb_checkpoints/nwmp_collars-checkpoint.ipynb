{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Open Source Queensland Geochemistry Data\n",
    "David Armstrong <br>\n",
    "Geological Survey of Queensland, Department of Resources <br>\n",
    "14/04/2022\n",
    "\n",
    "## Introduction\n",
    "The purpose of this notebook is to outline the current state of geochemistry data held by the Geologicaly Survey of Queenland (GSQ) and outline how to parse new data via GSQs API. Geochemistry data is mainly accessed through the [Queensland Exploration Geochemistry and Drillhole Database](https://geoscience.data.qld.gov.au/geochemistry/whole-of-queensland-geochemistry-databases) (will refer to this as the QLD Explorer database), which is a fantastic source with >5 million geochemistry data points. However, this database is static from its release in 2015. The GSQ Geoscience Data Modernisation Project (GDMP) introduced the framework and systems for standardised data submission and ETL pipelines. Therefore any data submitted 2020 onwards does not suffer from the historical and non-standardised issues we outline below. An overview of GDMP and data standardisation can be found [here](https://www.youtube.com/watch?v=Uv8FNk7SIF8). \n",
    "\n",
    "Hence updating the QLD Explorer Database is focusing on data from what we refer to as the 'ten year gap' between 2010-2020. We will frame the challenge of extracting data from this period and how it might be approached. All data and code in this notebook is open source via the GSQ Open Data Portal and API, providing a starting reference for anyone who is interested in attempting parsing tabular geochem data. \n",
    "\n",
    "#### Queensland Exploration Geochemistry and Drillhole Database (-2010)\n",
    "A guide to the compilation of the [QLD Explorer Database](https://geoscience.data.qld.gov.au/geochemistry/geochemistry-data-how-to-guide) provides a detailed summary of the compilation and structure of the database. Historically GSQ has received data submitted in report form via industry. There was inconsistent standards in the structure and formatting for this data. Therefore the QLD Explorer Database was manually compiled at a great expense, both financial and time (years) from >30,000 reports. This project was completed back in 2015 with open file reports, with the most recent report in the database dates to ~2010. The manual nature and resources required to build the database is why it is static and has not been updated. \n",
    "\n",
    "#### Ten Year Gap (2010-2020)\n",
    "In this period, geochemistry data has predominately been submitted as separate associated documents. Mainly text files, submission as associated documents has the advantage that we can easily write scripts to automatically parse the data. This report will demonstrate how to do this via the GSQ API. However, the data is still inoconsistent in the structure and format. Going from this unstructured to structured format is **the challenge** in combining the data. \n",
    "\n",
    "With the introduction of the GDMP, geochemistry data is now submitted in a standardised format, resolving the problems outlined above. It is still worth exploring the data submitted between 2010-2020, as it represents a significant sample size. In Figure 1, in green are soil samples from the QLD Explorers Database. In purple, are new soil samples that have been extracted associated documents following a similar method outlined in this report. This hopefully highlights the potential and value in attempting this process.  \n",
    "\n",
    "![](Picture1.png)\n",
    "\n",
    "*Figure 1. North West QLD soil samples. In green are samples from the QLD Explorer Database, and purple are samples extracted from associated documents*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1- Data Extraction via API\n",
    "Data can be downloaded from the GSQ Open Data Portal (ODP) via the API. Refer to the GSQ [Github](https://github.com/geological-survey-of-queensland/open-data-api) for doucmentation and examples. A detailed talk on the GSQ Open Data Poral and API can also be found [here](https://www.youtube.com/watch?v=o8gvrbZ-U8M).\n",
    "\n",
    "The API allows us to search for reports within a given area via a bounding box. We will be focusing on an area North East of Cloncurry  bbox = [140.2, -20.75, 141.5, -19.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import re\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reports in the Area 2502\n"
     ]
    }
   ],
   "source": [
    "# Using GSQ API, will search for the number of reports in the area.\n",
    "api = 'https://geoscience.data.qld.gov.au/api/3/action/'\n",
    "bbox = bbox = [140.2, -20.75, 141.5, -19.75]\n",
    "\n",
    "# Search using bounding box provided, and filter the query (fq) to reports only\n",
    "response = requests.get(api + 'package_search',\n",
    "                   params={\n",
    "                       'ext_bbox': bbox,\n",
    "                       'fq':['type:report']\n",
    "                   })\n",
    "\n",
    "print(\"Number of Reports in the Area\",response.json()['result']['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this area there are 2500 reports. The CKAN API has a maximum limit of returning 1000 datasets at a time. Therefore we will add on the extra row syntax to our API call, iterate through and store all of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "response_list = []\n",
    "while i <3000:\n",
    "    rows_start = i\n",
    "    rows_end = i + 1000\n",
    "    response = requests.get(api + 'package_search'+'?rows={0}&start={1}'.format(rows_end,rows_start),\n",
    "                   params={\n",
    "                       'ext_bbox': bbox,\n",
    "                       'fq':[\n",
    "                           'type:report']\n",
    "                   })\n",
    "    response_list.append(response.json()['result']['results'])\n",
    "    i = i + 1000\n",
    "\n",
    "report_list = sum(response_list, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now have all 2500 reports for the region stored in the response_list. For each report, a range of key meta data is returned with it, including a list of the associated documents under 'resources'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of key metadata extracted from the reponse json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['GeoJSONextent', 'author', 'author_email', 'commodity', 'creator', 'creator_user_id', 'dataset_completion_date', 'dataset_start_date', 'earth_science_data_category', 'extra:access_rights', 'extra:contact_uri', 'extra:identifier', 'georesource_report_type', 'id', 'isopen', 'license_id', 'license_title', 'license_url', 'maintainer', 'maintainer_email', 'metadata_created', 'metadata_modified', 'name', 'notes', 'num_resources', 'num_tags', 'open_file_date', 'organization', 'owner', 'owner_org', 'private', 'resource_authority_permit', 'spatial', 'state', 'syndicate', 'title', 'type', 'url', 'version', 'resources', 'tags', 'groups', 'relationships_as_subject', 'relationships_as_object'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"List of key metadata extracted from the reponse json\")\n",
    "report_list[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next stage is to filer out for reports that we are interested in. We are specifically interested in reports that have drillhole details. From experience, these are normally submitted as 'collar.txt' files. The associated documents with a report can be found under 'resources' in the API response. The second filter we will apply is for any reports with an open_file_date from 2010 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of collar files found 164\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each report\n",
    "collar_file_dict = {}\n",
    "for report in report_list:\n",
    "    meta_data = report.keys()\n",
    "    # The response must have resources and a open file date to meet our filtering criteria.\n",
    "    if 'resources' in meta_data and 'open_file_date' in meta_data:\n",
    "        if report['open_file_date'] >= '2010-01-01':\n",
    "            # Get the names of the reources files\n",
    "            for resource in report['resources']:\n",
    "                name = resource['name'].lower()\n",
    "                if 'collar' in name:\n",
    "                    collar_file_dict[report['extra:identifier']] = resource['url']\n",
    "            \n",
    "print(\"Number of collar files found\", len(collar_file_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the 2500 original reports, we found 164 which had been open filed in the past year, and had an associated document with 'collar' in it. This seems consistent for a relatively small area set by the bounding box. The next stage will be to analyse these files. Note, all of GSQ open file data is stored on an Amazon S3 bucket. This effectively acts as free storage. In the resources response we stored the 'url' which gives the S3 location for that file, which can be read directly from there without having to download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Collar File Analysis\n",
    "We will look at the format of the different collar files. Iterating through, first will see the breakdown of files in the GGIC format or 'other'. If in GGIC format the locations should be standardized and easy to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggic = {}\n",
    "other = {}\n",
    "error_reading_file = 0\n",
    "for key, value in collar_file_dict.items():\n",
    "    try:\n",
    "        resource_url = \"\"\"{}\"\"\".format(value)\n",
    "        response = urllib.request.urlopen(resource_url).read(500).decode('utf-8')\n",
    "        response = response.replace('\\t',' ')\n",
    "        response = response.replace('\\r','')\n",
    "        if 'H0001'  in response or 'H0002' in response:\n",
    "            ggic[key] = value\n",
    "        else:\n",
    "            other[key] = value\n",
    "    except:\n",
    "        error_reading_file = error_reading_file+1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GGIC templates  79\n",
      "Number of other templates 75\n",
      "Error reading file 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of GGIC templates \", len(ggic))\n",
    "print(\"Number of other templates\", len(other))\n",
    "print(\"Error reading file\", error_reading_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half the files appear to be in GGIC format. That's good news, as this is a standardised format and depending on the version should be able to extract standardised locations from. Details on file structure can be found [here](https://www.australiaminerals.gov.au/__data/assets/pdf_file/0004/60772/National_Guidelines_Version_4.5_February_18.pdf). The error reading files might be due to file format, delimited files, encoding etc. Will ignore for the moment. Below we will explore the remaining 'other' files, which were not in GGIC format. From experience, know that the majority of files are submitted as tab delimited files. There will be some exceptions to this, and we will throw an error for any problems encounted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3. Non-standard files\n",
    "We will explore the structure and naming conventions of these non-structured files, and see if can at least find the locations of the drillholes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file  CR052357\n"
     ]
    }
   ],
   "source": [
    "sub_df_list = []\n",
    "for key, value in other.items():\n",
    "    try:\n",
    "        resource_url = \"\"\"{}\"\"\".format(value)\n",
    "        sub_df = pd.read_csv(resource_url, encoding= 'latin',sep= '\\t')\n",
    "        sub_df_list.append(sub_df)\n",
    "    except:\n",
    "        print(\"Error reading file \", key)\n",
    "final = pd.concat(sub_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different columns found in non-GGIC files 452\n",
      " \n",
      "Index(['Drillhole Number', 'Easting', 'Northing', 'Dip', 'Azimuth',\n",
      "       'Precollar depth (m)', 'Total depth (m)', 'Property', 'EPM number',\n",
      "       'Surveys',\n",
      "       ...\n",
      "       'U3O8max', 'int', 'gmU3O8', 'Hold ID', 'Depth\\n/Pre-Collar', 'Az_(mag)',\n",
      "       'Completion\\n Status', 'H0100', 'Tenement_name', 'EPM12409'],\n",
      "      dtype='object', length=452)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of different columns found in non-GGIC files\", len(final.columns))\n",
    "print(\" \")\n",
    "print(final.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 75 collar files in non GGIC format, there were ~452 unique column names. Standardising this output represents the challenge in creating tabular format. We will attempt to extract just the locations. Whilst these could be named anything and in different datums, we can test the hypothesis that if a column's values are decimals to 6-7 places then these fields will correspond to location values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locations Extracted CR038894\n",
      "No locations found based on values being decmials of 6 significant places CR045577\n",
      "Locations Extracted CR065942\n",
      "Locations Extracted CR054886\n",
      "Locations Extracted CR050712\n",
      "Locations Extracted CR047152\n",
      "No locations found based on values being decmials of 6 significant places CR064492\n",
      "No locations found based on values being decmials of 6 significant places CR053412\n",
      "Locations Extracted CR059724\n",
      "No locations found based on values being decmials of 6 significant places CR115338\n",
      "Locations Extracted CR077011\n",
      "Locations Extracted CR099936\n",
      "Locations Extracted CR117573\n",
      "No locations found based on values being decmials of 6 significant places CR072781\n",
      "Locations Extracted CR037386\n",
      "Locations Extracted CR062798\n",
      "Locations Extracted CR083057\n",
      "Locations Extracted CR046729\n",
      "Locations Extracted CR076832\n",
      "Locations Extracted CR071781\n",
      "Locations Extracted CR067288\n",
      "Locations Extracted CR090938\n",
      "Locations Extracted CR058080\n",
      "Locations Extracted CR052204\n",
      "Locations Extracted CR088463\n",
      "Locations Extracted CR102526\n",
      "Locations Extracted CR085622\n",
      "Locations Extracted CR099052\n",
      "Locations Extracted CR062166\n",
      "Locations Extracted CR083288\n",
      "Locations Extracted CR112394\n",
      "Locations Extracted CR042947\n",
      "No locations found based on values being decmials of 6 significant places CR096582\n",
      "Locations Extracted CR088462\n",
      "No locations found based on values being decmials of 6 significant places CR040068\n",
      "Locations Extracted CR074076\n",
      "Locations Extracted CR069343\n",
      "Locations Extracted CR067464\n",
      "Error reading file  CR052357\n",
      "No locations found based on values being decmials of 6 significant places CR057829\n",
      "No locations found based on values being decmials of 6 significant places CR056605\n",
      "No locations found based on values being decmials of 6 significant places CR098064\n",
      "Locations Extracted CR100172\n",
      "No locations found based on values being decmials of 6 significant places CR067452\n",
      "No locations found based on values being decmials of 6 significant places CR078299\n",
      "No locations found based on values being decmials of 6 significant places CR071805\n",
      "No locations found based on values being decmials of 6 significant places CR070206\n",
      "Locations Extracted CR042783\n",
      "Locations Extracted CR069615\n",
      "Locations Extracted CR072600\n",
      "Locations Extracted CR069163\n",
      "Locations Extracted CR047492\n",
      "Locations Extracted CR099386\n",
      "Locations Extracted CR039035\n",
      "No locations found based on values being decmials of 6 significant places CR050098\n",
      "No locations found based on values being decmials of 6 significant places CR053103\n",
      "No locations found based on values being decmials of 6 significant places CR085254\n",
      "No locations found based on values being decmials of 6 significant places CR063905\n",
      "Locations Extracted CR050078\n",
      "Locations Extracted CR044443\n",
      "Locations Extracted CR096766\n",
      "Locations Extracted CR101506\n",
      "Locations Extracted CR094994\n",
      "No locations found based on values being decmials of 6 significant places CR079092\n",
      "Locations Extracted CR096397\n",
      "Locations Extracted CR099213\n",
      "Locations Extracted CR083042\n",
      "No locations found based on values being decmials of 6 significant places CR058941\n",
      "No locations found based on values being decmials of 6 significant places CR064657\n",
      "Locations Extracted CR053413\n",
      "Locations Extracted CR065719\n",
      "Locations Extracted CR054501\n",
      "No locations found based on values being decmials of 6 significant places CR054578\n",
      "Locations Extracted CR060989\n",
      "No locations found based on values being decmials of 6 significant places CR075414\n"
     ]
    }
   ],
   "source": [
    "sub_df_list = []\n",
    "for key, value in other.items():\n",
    "    resource_url = \"\"\"{}\"\"\".format(value)\n",
    "    try:\n",
    "        sub_df = pd.read_csv(resource_url, encoding= 'latin',sep= '\\t')\n",
    "        \n",
    "        # Drop duplicate columns\n",
    "        sub_df.columns = [x.lower() for x in sub_df.columns]\n",
    "        sub_df = sub_df.loc[:,~sub_df.columns.duplicated()]\n",
    "\n",
    "        numbered_columns = sub_df.select_dtypes(np.number).columns.tolist()\n",
    "        potential_ids = [x for x in sub_df.columns.tolist() if \"hole\" in x.lower()]\n",
    "        potential_locations = []\n",
    "        \n",
    "        for k in numbered_columns:\n",
    "            length = sub_df[k].astype(str).apply(len).median()\n",
    "            if length >= 5.0:\n",
    "                potential_locations.append(k)\n",
    "        \n",
    "        if len(potential_locations) > 0:\n",
    "            sub_df['report_id'] = key\n",
    "            sub_df = sub_df[['report_id']+potential_locations+potential_ids]\n",
    "            sub_df = sub_df.loc[:,~sub_df.columns.duplicated()]\n",
    "            sub_df_a = pd.melt(sub_df, id_vars=['report_id'], value_vars=potential_locations)\n",
    "            \n",
    "            sub_df_list.append(sub_df_a)\n",
    "            print('Locations Extracted', key)\n",
    "        if len(potential_locations) == 0:\n",
    "            print(\"No locations found based on values being decmials of 6 significant places\",key)\n",
    "    except:\n",
    "        print(\"Error reading file \", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential locations extracted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['easting', 'northing', 'east_mga', 'north_mga', 'nat_east',\n",
       "       'nat_north', 'amg54_84_x', 'amg54_84_y', 'east', 'north', 'nat_rl',\n",
       "       'amg e', 'amg n', 'rl', 'amg54_84_z', 'local_east', 'local_north',\n",
       "       'local_rl', 'orig_east', 'orig_north', 'mga_east', 'mga_north',\n",
       "       'll_lat', 'll_long', 'working_easting', 'working_northing',\n",
       "       'amg_east', 'amg_north', 'mga_rl', 'll_rl', 'easting amg66z54',\n",
       "       'northing amg66z54', 'survey_east', 'survey_north', 'date', 'mgae',\n",
       "       'mgan', 'me', 'mn', 'x', 'y', 'local_e', 'local_n', 'amg_n',\n",
       "       'amg_e', 'latitude', 'longitude', 'easting gda94',\n",
       "       'northing gda94', 'gda_e', 'gda_n', 'gda94_easting',\n",
       "       'gda94_northing'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = pd.concat(sub_df_list)\n",
    "print(\"Potential locations extracted\")\n",
    "final['variable'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attempt to extract location based on finding columns with a 5 or more decimal places didn't work for every collar file. However as seen above, the columns that were extracted do correspond to locations. This, combined with the GGIC formats means we should be able to find locations for a significant number of drillholes and their locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next stage would be a data wrangling exercise of mapping the extracted locations above to consistent format, and parsing through GGIC submitted data. However the location is just the first stage, this will then have to be combined with analytical and survey information. These are again submitted in separate assocaited documents, but should be able to be identified based on file names like above.\n",
    "\n",
    "For surface samples, often files will be named as some variation of 'rock chip, stream sediment, soil' or common abbreviations. There will be a mixture of GGIC and other formats. Creating a complete compilation of data via scripting seems unlikely, however it could automate a large proportion of the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c798a58345a0e27b82703fadc3879a7155cd5a12be918ccf01c752d475bc279"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
